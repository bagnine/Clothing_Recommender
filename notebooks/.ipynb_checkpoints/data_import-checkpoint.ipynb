{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Data\n",
    "\n",
    "The purpose of this notebook is to web scrape data from grailed.com in order to generate a DataFrame for analysis and modeling. First, I took summary information and url links from the main page after automating a browser to scroll and refresh the screen 200 times. Next, I took the URLs I pulled from the original page and scraped more complete info on each listing, plus the top image. Finally, I joined the resulting DataFrames and saved for import into my EDA notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import progressbar\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/nicksubic/Documents/flatiron/phase_1/nyc-mhtn-ds-091420-lectures/capstone/Clothing_Recommender/src/')\n",
    "sys.path.append('/Users/nicksubic/.wdm/drivers/chromedriver/mac64/87.0.4280.88/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Trying to download new driver from http://chromedriver.storage.googleapis.com/87.0.4280.88/chromedriver_mac64.zip\n",
      " \n",
      "[WDM] - Driver has been saved in cache [/Users/nicksubic/.wdm/drivers/chromedriver/mac64/87.0.4280.88]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the webdriver\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instatiating the webdriver and scrolling to update the page 200 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "# Navigate to the main page for tops\n",
    "driver.get('https://www.grailed.com/categories/tops')\n",
    "timeout = 30\n",
    "\n",
    "#Wait for the page to load or return a Time Out error and close\n",
    "try:\n",
    "    WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='feed-item']\")))\n",
    "except TimeoutException:\n",
    "    print(\"Timed Out\")\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_page(number_of_scrolls):\n",
    "    '''Takes in a number of scrolls and scrolls to up the selected page that number of times'''\n",
    "    # Bypass the header and recommended items\n",
    "    results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "\n",
    "    # Start the scroll\n",
    "    for i in range(0, number_of_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        \n",
    "        # Print an update every 30 scrolls\n",
    "        if i%30 == 0:\n",
    "            print(f'Completed scroll {i+1} out of {number_of_scrolls}')\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Set the results as everything after the initial scroll\n",
    "    results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scroll_page(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the first DataFrame by scraping the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.make_dataframe(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10100 entries, 0 to 10099\n",
      "Data columns (total 9 columns):\n",
      "Name        10100 non-null object\n",
      "Designer    10100 non-null object\n",
      "Price       3658 non-null object\n",
      "NewPrice    6442 non-null object\n",
      "OldPrice    6442 non-null object\n",
      "Size        10100 non-null object\n",
      "Time        10100 non-null object\n",
      "LastBump    7759 non-null object\n",
      "Link        10100 non-null object\n",
      "dtypes: object(9)\n",
      "memory usage: 710.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the second DataFrame by following the links in the first one, gathering more info and downloading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_dataframe(list_of_links):\n",
    "    '''Takes a list of links to grailed.com and scrapes data and saves the main image.\n",
    "       Returns a DataFrame with all the page's summary information.'''\n",
    "\n",
    "    # Lists to append everything to\n",
    "    UserName=[]\n",
    "    Sold=[]\n",
    "    Feedback=[]\n",
    "    CurrentListings=[]\n",
    "    Description=[]\n",
    "    ProfileLink=[]\n",
    "    FeedBack=[]\n",
    "    FeedbackLink=[]\n",
    "    FollowerCount=[]\n",
    "    FullSize=[]\n",
    "    PostedTime=[]\n",
    "    BumpedTime=[]\n",
    "    Location=[]\n",
    "    Transactions=[]\n",
    "\n",
    "    # Iterate through the list of links\n",
    "    for i, link in enumerate(df.Link):\n",
    "        \n",
    "        # If the link is broken, add a nan value to every column in that row\n",
    "        try:\n",
    "            driver.get(link)\n",
    "        except HTTPError:\n",
    "            UserName.append(np.nan)\n",
    "            Sold.append(np.nan)\n",
    "            Feedback.append(np.nan)\n",
    "            CurrentListings.append(np.nan)\n",
    "            Description.append(np.nan)\n",
    "            ProfileLink.append(np.nan)\n",
    "            FeedBack.append(np.nan)\n",
    "            FeedbackLink.append(np.nan)\n",
    "            FollowerCount.append(np.nan)\n",
    "            FullSize.append(np.nan)\n",
    "            PostedTime.append(np.nan)\n",
    "            BumpedTime.append(np.nan)\n",
    "            Location.append(np.nan)\n",
    "            Transactions.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        # Download the first image for the listing, start the loop over if it's missing\n",
    "        try:\n",
    "            image = driver.find_element_by_xpath('//div[@class=\"-image-wrapper\"]')\n",
    "            src = image.find_element_by_tag_name('img').get_attribute('src')\n",
    "            urllib.request.urlretrieve(src, f'src/images/{i}.jpg')\n",
    "        except NoSuchElementException:\n",
    "            continue\n",
    "        \n",
    "        # Get the seller's name, or input NaN if there isn't one\n",
    "        try:\n",
    "            UserName.append(driver.find_element_by_xpath('//span[@class=\"-username\"]').text)\n",
    "        except NoSuchElementException:\n",
    "            UserName.append(np.nan)\n",
    "\n",
    "        # Get number of items sold by seller, else NaN\n",
    "        try:\n",
    "            Sold.append(driver.find_element_by_xpath('//a[@class=\"-link\"]/span[2]').text)\n",
    "        except NoSuchElementException:\n",
    "            Sold.append(np.nan)\n",
    "\n",
    "        # Get number Feedback ratings seller has\n",
    "        try:\n",
    "            FeedBack.append(driver.find_element_by_xpath('//span[@class=\"-feedback-count\"]').text)\n",
    "        except NoSuchElementException:\n",
    "            FeedBack.append(np.nan)\n",
    "\n",
    "        # Get number of seller's listings\n",
    "        try:\n",
    "            CurrentListings.append(driver.find_element_by_xpath('//a[@class=\"-for-sale-link\"]').text)\n",
    "        except NoSuchElementException:\n",
    "            CurrentListings.append(np.nan)\n",
    "\n",
    "        # Get Item Description- this is a pretty long string generally\n",
    "        try:\n",
    "            Description.append(driver.find_element_by_xpath('//div[@class=\"listing-description\"]').text)\n",
    "        except NoSuchElementException:\n",
    "            Description.append(np.nan)\n",
    "\n",
    "        # Get seller's profile link\n",
    "        try:\n",
    "            ProfileLink.append(driver.find_element_by_xpath('//span[@class=\"Username\"]/a').get_attribute(\"href\"))\n",
    "        except NoSuchElementException:\n",
    "            ProfileLink.append(np.nan)\n",
    "\n",
    "        # Get Feedback Link\n",
    "        try:\n",
    "            FeedbackLink.append(driver.find_element_by_xpath('//div[@class=\"-details\"]/a').get_attribute(\"href\"))\n",
    "        except NoSuchElementException:\n",
    "            FeedbackLink.append(np.nan)\n",
    "\n",
    "        # Get number of likes for item\n",
    "        try:\n",
    "            FollowerCount.append(driver.find_element_by_xpath('//p[@class=\"-follower-count\"]').text)\n",
    "        except NoSuchElementException:\n",
    "            FollowerCount.append(np.nan)\n",
    "\n",
    "        # Get expanded size info\n",
    "        try:\n",
    "            FullSize.append(driver.find_element_by_xpath('//h2[@class=\"listing-size sub-title\"]').text)\n",
    "        except NoSuchElementException:\n",
    "            FullSize.append(np.nan)\n",
    "\n",
    "        # Get time posted\n",
    "        try:\n",
    "            PostedTime.append(driver.find_element_by_xpath('//div[@class=\"-metadata\"]/span[2]').text)\n",
    "        except NoSuchElementException:\n",
    "            PostedTime.append(np.nan)\n",
    "\n",
    "        # Get time price dropped most recently\n",
    "        try:\n",
    "            BumpedTime.append(driver.find_element_by_xpath('//div[@class=\"-metadata\"]/span[4]').text)\n",
    "        except NoSuchElementException:\n",
    "            BumpedTime.append(np.nan)\n",
    "\n",
    "        # Get seller location\n",
    "        try:\n",
    "            Location.append(driver.find_element_by_xpath('//label[@class=\"--label\"]').text)\n",
    "        except NoSuchElementException:\n",
    "            Location.append(np.nan)\n",
    "        \n",
    "        # Print status update every 100 pages\n",
    "        if i%100 == 0:\n",
    "            print(f'Completed Page {i} out of {len(list_of_links)}.')\n",
    "\n",
    "    # Create dictionary with every list generated by the scrape loop   \n",
    "    page_dict = {'Username': UserName,\n",
    "                     'Sold': Sold,\n",
    "                     'Feedback': FeedBack,\n",
    "                     'CurrentListings': CurrentListings, \n",
    "                     'Description': Description, \n",
    "                     'ProfileLink': ProfileLink, \n",
    "                     'FeedbackLink': FeedbackLink, \n",
    "                     'FollowerCount': FollowerCount,\n",
    "                     'FullSize': FullSize,\n",
    "                     'PostedTime': PostedTime,\n",
    "                     'BumpedTime': BumpedTime, \n",
    "                     'Location': Location}\n",
    "\n",
    "    # Convert the dictionary to a DataFrame and return    \n",
    "    return pd.DataFrame(page_dict), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Page 0 out of 10100.\n",
      "Completed Page 100 out of 10100.\n",
      "Completed Page 200 out of 10100.\n",
      "Completed Page 300 out of 10100.\n",
      "Completed Page 400 out of 10100.\n",
      "Completed Page 500 out of 10100.\n",
      "Completed Page 600 out of 10100.\n",
      "Completed Page 700 out of 10100.\n",
      "Completed Page 800 out of 10100.\n",
      "Completed Page 900 out of 10100.\n",
      "Completed Page 1000 out of 10100.\n",
      "Completed Page 1100 out of 10100.\n",
      "Completed Page 1200 out of 10100.\n",
      "Completed Page 1300 out of 10100.\n",
      "Completed Page 1400 out of 10100.\n",
      "Completed Page 1500 out of 10100.\n",
      "Completed Page 1600 out of 10100.\n",
      "Completed Page 1700 out of 10100.\n",
      "Completed Page 1800 out of 10100.\n",
      "Completed Page 1900 out of 10100.\n",
      "Completed Page 2000 out of 10100.\n",
      "Completed Page 2100 out of 10100.\n",
      "Completed Page 2200 out of 10100.\n",
      "Completed Page 2300 out of 10100.\n",
      "Completed Page 2400 out of 10100.\n",
      "Completed Page 2500 out of 10100.\n",
      "Completed Page 2600 out of 10100.\n",
      "Completed Page 2700 out of 10100.\n",
      "Completed Page 2800 out of 10100.\n",
      "Completed Page 2900 out of 10100.\n",
      "Completed Page 3000 out of 10100.\n",
      "Completed Page 3100 out of 10100.\n",
      "Completed Page 3200 out of 10100.\n",
      "Completed Page 3300 out of 10100.\n",
      "Completed Page 3400 out of 10100.\n",
      "Completed Page 3500 out of 10100.\n",
      "Completed Page 3600 out of 10100.\n",
      "Completed Page 3700 out of 10100.\n",
      "Completed Page 3800 out of 10100.\n",
      "Completed Page 3900 out of 10100.\n",
      "Completed Page 4000 out of 10100.\n",
      "Completed Page 4100 out of 10100.\n",
      "Completed Page 4200 out of 10100.\n",
      "Completed Page 4300 out of 10100.\n",
      "Completed Page 4400 out of 10100.\n",
      "Completed Page 4500 out of 10100.\n",
      "Completed Page 4600 out of 10100.\n",
      "Completed Page 4700 out of 10100.\n",
      "Completed Page 4800 out of 10100.\n",
      "Completed Page 4900 out of 10100.\n",
      "Completed Page 5000 out of 10100.\n",
      "Completed Page 5100 out of 10100.\n",
      "Completed Page 5200 out of 10100.\n",
      "Completed Page 5300 out of 10100.\n",
      "Completed Page 5400 out of 10100.\n",
      "Completed Page 5500 out of 10100.\n",
      "Completed Page 5600 out of 10100.\n",
      "Completed Page 5700 out of 10100.\n",
      "Completed Page 5800 out of 10100.\n",
      "Completed Page 5900 out of 10100.\n",
      "Completed Page 6000 out of 10100.\n",
      "Completed Page 6100 out of 10100.\n",
      "Completed Page 6200 out of 10100.\n",
      "Completed Page 6300 out of 10100.\n",
      "Completed Page 6400 out of 10100.\n",
      "Completed Page 6500 out of 10100.\n",
      "Completed Page 6600 out of 10100.\n",
      "Completed Page 6700 out of 10100.\n",
      "Completed Page 6800 out of 10100.\n",
      "Completed Page 6900 out of 10100.\n",
      "Completed Page 7000 out of 10100.\n",
      "Completed Page 7100 out of 10100.\n",
      "Completed Page 7200 out of 10100.\n",
      "Completed Page 7300 out of 10100.\n",
      "Completed Page 7400 out of 10100.\n",
      "Completed Page 7500 out of 10100.\n",
      "Completed Page 7600 out of 10100.\n",
      "Completed Page 7700 out of 10100.\n",
      "Completed Page 7800 out of 10100.\n",
      "Completed Page 7900 out of 10100.\n",
      "Completed Page 8000 out of 10100.\n",
      "Completed Page 8100 out of 10100.\n",
      "Completed Page 8200 out of 10100.\n",
      "Completed Page 8300 out of 10100.\n",
      "Completed Page 8400 out of 10100.\n",
      "Completed Page 8500 out of 10100.\n",
      "Completed Page 8600 out of 10100.\n",
      "Completed Page 8700 out of 10100.\n",
      "Completed Page 8800 out of 10100.\n",
      "Completed Page 8900 out of 10100.\n",
      "Completed Page 9000 out of 10100.\n",
      "Completed Page 9100 out of 10100.\n",
      "Completed Page 9200 out of 10100.\n",
      "Completed Page 9300 out of 10100.\n",
      "Completed Page 9400 out of 10100.\n",
      "Completed Page 9500 out of 10100.\n",
      "Completed Page 9600 out of 10100.\n",
      "Completed Page 9700 out of 10100.\n",
      "Completed Page 9800 out of 10100.\n",
      "Completed Page 9900 out of 10100.\n",
      "Completed Page 10000 out of 10100.\n"
     ]
    }
   ],
   "source": [
    "df2 = page_dataframe(df.Link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close webdriver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the DataFrames and verifying everything worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the DataFrames\n",
    "df3 = pd.concat([df, df2[0]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Designer</th>\n",
       "      <th>Price</th>\n",
       "      <th>NewPrice</th>\n",
       "      <th>OldPrice</th>\n",
       "      <th>Size</th>\n",
       "      <th>Time</th>\n",
       "      <th>LastBump</th>\n",
       "      <th>Link</th>\n",
       "      <th>Username</th>\n",
       "      <th>...</th>\n",
       "      <th>Feedback</th>\n",
       "      <th>CurrentListings</th>\n",
       "      <th>Description</th>\n",
       "      <th>ProfileLink</th>\n",
       "      <th>FeedbackLink</th>\n",
       "      <th>FollowerCount</th>\n",
       "      <th>FullSize</th>\n",
       "      <th>PostedTime</th>\n",
       "      <th>BumpedTime</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bape Varsity style Jacket Bathing Ape</td>\n",
       "      <td>BAPE</td>\n",
       "      <td>$155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>about 14 hours ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.grailed.com/listings/18766603-bape...</td>\n",
       "      <td>oghypeshop</td>\n",
       "      <td>...</td>\n",
       "      <td>77 Feedback</td>\n",
       "      <td>59 Listings for Sale</td>\n",
       "      <td>Bape Varsity Jacket\\nSize M fits true\\nRelease...</td>\n",
       "      <td>https://www.grailed.com/oghypeshop</td>\n",
       "      <td>https://www.grailed.com/oghypeshop/feedback</td>\n",
       "      <td>111</td>\n",
       "      <td>Size: US M / EU 48-50 / 2</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Add a comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vintage Nike Sunfaded Mini Swoosh Travis Style...</td>\n",
       "      <td>MADE IN USA × NIKE × VINTAGE</td>\n",
       "      <td>$100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L</td>\n",
       "      <td>about 17 hours ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.grailed.com/listings/18763620-made...</td>\n",
       "      <td>im_groot</td>\n",
       "      <td>...</td>\n",
       "      <td>8 Feedback</td>\n",
       "      <td>47 Listings for Sale</td>\n",
       "      <td>Brand : Vintage Nike sunfaded mini swoosh blac...</td>\n",
       "      <td>https://www.grailed.com/im_groot</td>\n",
       "      <td>https://www.grailed.com/im_groot/feedback</td>\n",
       "      <td>87</td>\n",
       "      <td>Size: US L / EU 52-54 / 3</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shipping: Asia to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CDG logo hoodie</td>\n",
       "      <td>CDG CDG CDG × COMME DES GARCONS</td>\n",
       "      <td>$129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L</td>\n",
       "      <td>about 18 hours ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.grailed.com/listings/18761921-cdg-...</td>\n",
       "      <td>binefartoldn</td>\n",
       "      <td>...</td>\n",
       "      <td>25 Feedback</td>\n",
       "      <td>9 Listings for Sale</td>\n",
       "      <td>Comes des garçons black hoodie\\nFrom Dover str...</td>\n",
       "      <td>https://www.grailed.com/binefartoldn</td>\n",
       "      <td>https://www.grailed.com/binefartoldn/feedback</td>\n",
       "      <td>168</td>\n",
       "      <td>Size: US L / EU 52-54 / 3</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shipping: UK to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90s Faded Uni Blank Tee</td>\n",
       "      <td>STREETWEAR × VINTAGE</td>\n",
       "      <td>$25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.grailed.com/listings/18750493-stre...</td>\n",
       "      <td>TwoFold</td>\n",
       "      <td>...</td>\n",
       "      <td>1086 Feedback</td>\n",
       "      <td>189 Listings for Sale</td>\n",
       "      <td>90s Faded Uni Blank Tee. Size Medium.\\nPit To ...</td>\n",
       "      <td>https://www.grailed.com/TwoFold</td>\n",
       "      <td>https://www.grailed.com/TwoFold/feedback</td>\n",
       "      <td>107</td>\n",
       "      <td>Size: US M / EU 48-50 / 2</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shipping: US to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vintage Nike Grey Black Big Logo Spellout Hood...</td>\n",
       "      <td>NIKE × VINTAGE</td>\n",
       "      <td>$55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.grailed.com/listings/18745492-nike...</td>\n",
       "      <td>gcwiek</td>\n",
       "      <td>...</td>\n",
       "      <td>275 Feedback</td>\n",
       "      <td>46 Listings for Sale</td>\n",
       "      <td>Mens medium\\nGood condition other than some we...</td>\n",
       "      <td>https://www.grailed.com/gcwiek</td>\n",
       "      <td>https://www.grailed.com/gcwiek/feedback</td>\n",
       "      <td>108</td>\n",
       "      <td>Size: US M / EU 48-50 / 2</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shipping: US to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name  \\\n",
       "0              Bape Varsity style Jacket Bathing Ape   \n",
       "1  Vintage Nike Sunfaded Mini Swoosh Travis Style...   \n",
       "2                                    CDG logo hoodie   \n",
       "3                            90s Faded Uni Blank Tee   \n",
       "4  Vintage Nike Grey Black Big Logo Spellout Hood...   \n",
       "\n",
       "                          Designer Price NewPrice OldPrice Size  \\\n",
       "0                             BAPE  $155      NaN      NaN    M   \n",
       "1     MADE IN USA × NIKE × VINTAGE  $100      NaN      NaN    L   \n",
       "2  CDG CDG CDG × COMME DES GARCONS  $129      NaN      NaN    L   \n",
       "3             STREETWEAR × VINTAGE   $25      NaN      NaN    M   \n",
       "4                   NIKE × VINTAGE   $55      NaN      NaN    M   \n",
       "\n",
       "                 Time LastBump  \\\n",
       "0  about 14 hours ago      NaN   \n",
       "1  about 17 hours ago      NaN   \n",
       "2  about 18 hours ago      NaN   \n",
       "3           1 day ago      NaN   \n",
       "4           1 day ago      NaN   \n",
       "\n",
       "                                                Link      Username  ...  \\\n",
       "0  https://www.grailed.com/listings/18766603-bape...    oghypeshop  ...   \n",
       "1  https://www.grailed.com/listings/18763620-made...      im_groot  ...   \n",
       "2  https://www.grailed.com/listings/18761921-cdg-...  binefartoldn  ...   \n",
       "3  https://www.grailed.com/listings/18750493-stre...       TwoFold  ...   \n",
       "4  https://www.grailed.com/listings/18745492-nike...        gcwiek  ...   \n",
       "\n",
       "        Feedback        CurrentListings  \\\n",
       "0    77 Feedback   59 Listings for Sale   \n",
       "1     8 Feedback   47 Listings for Sale   \n",
       "2    25 Feedback    9 Listings for Sale   \n",
       "3  1086 Feedback  189 Listings for Sale   \n",
       "4   275 Feedback   46 Listings for Sale   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Bape Varsity Jacket\\nSize M fits true\\nRelease...   \n",
       "1  Brand : Vintage Nike sunfaded mini swoosh blac...   \n",
       "2  Comes des garçons black hoodie\\nFrom Dover str...   \n",
       "3  90s Faded Uni Blank Tee. Size Medium.\\nPit To ...   \n",
       "4  Mens medium\\nGood condition other than some we...   \n",
       "\n",
       "                            ProfileLink  \\\n",
       "0    https://www.grailed.com/oghypeshop   \n",
       "1      https://www.grailed.com/im_groot   \n",
       "2  https://www.grailed.com/binefartoldn   \n",
       "3       https://www.grailed.com/TwoFold   \n",
       "4        https://www.grailed.com/gcwiek   \n",
       "\n",
       "                                    FeedbackLink FollowerCount  \\\n",
       "0    https://www.grailed.com/oghypeshop/feedback           111   \n",
       "1      https://www.grailed.com/im_groot/feedback            87   \n",
       "2  https://www.grailed.com/binefartoldn/feedback           168   \n",
       "3       https://www.grailed.com/TwoFold/feedback           107   \n",
       "4        https://www.grailed.com/gcwiek/feedback           108   \n",
       "\n",
       "                    FullSize  PostedTime BumpedTime           Location  \n",
       "0  Size: US M / EU 48-50 / 2   1 day ago        NaN      Add a comment  \n",
       "1  Size: US L / EU 52-54 / 3   1 day ago        NaN  Shipping: Asia to  \n",
       "2  Size: US L / EU 52-54 / 3   1 day ago        NaN    Shipping: UK to  \n",
       "3  Size: US M / EU 48-50 / 2  2 days ago        NaN    Shipping: US to  \n",
       "4  Size: US M / EU 48-50 / 2  2 days ago        NaN    Shipping: US to  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "df3.to_csv('./src/data/grailed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info is now scraped, combined and ready to load into the EDA notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
