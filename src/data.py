import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
import time
import urllib
import numpy as np

def scroll_page(number_of_scrolls):
    '''Scrolls down to refresh the page a specified number of times'''
    # Skip the header and go straight into the main feed
    results = driver.find_elements_by_xpath('//div[@class="FiltersInstantSearch"]//div[@class="feed-item"]')

    # Scroll the specified number of times with a 2 second break in between
    for i in range(0,number_of_scrolls):
        driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")
        print(f'Completed scroll {i+1} out of {number_of_scrolls)}')
        time.sleep(2)

    # Creates an object containing indices for every item on the page after scrolling
    results = driver.find_elements_by_xpath('//div[@class="FiltersInstantSearch"]//div[@class="feed-item"]')
    return results

def make_dataframe(results):
    '''Takes in the results from the scroll function, iterates through them
       and pulls data from each of the following categories: Name, Designer, 
       Price, (New Price and Old Price if an item has changed prices), Size,
       Time (when an item was posted), Last Bump (when the price dropped most
       recently) and Link.
       
       The output is a DataFrame with each of the features as a column'''

    Name = []
    Designer = []
    Price = []
    NewPrice = []
    OldPrice = []
    Size = []
    Time = []
    LastBump = []
    Link = []

    for i, result in enumerate(results):

        Designer.append(result.find_element_by_class_name("listing-designer").text)
        Name.append(result.find_element_by_class_name("listing-title").text)
        
        try:
            Price.append(result.find_element_by_xpath('.//p[@class="sub-title original-price"]').text)
            NewPrice.append(np.nan)
            OldPrice.append(np.nan)
        except NoSuchElementException:
            NewPrice.append(result.find_element_by_xpath('.//p[@class="sub-title new-price"]').text)
            OldPrice.append(result.find_element_by_xpath('.//p[@class="sub-title original-price strike-through"]').text)
            Price.append(np.nan)

        Size.append(result.find_element_by_xpath('.//p[@class="listing-size sub-title"]').text)

        Time.append(result.find_element_by_xpath(".//span[@class='date-ago']").text)

        try:
            LastBump.append(result.find_element_by_xpath(".//span[@class='strike-through']").text)
        except NoSuchElementException:
            LastBump.append(np.nan)

        Link.append(result.find_element_by_xpath('./a').get_attribute("href"))

        grailed_dict = {'Name': Name, 
                    'Designer': Designer, 
                    'Price': Price, 
                    'NewPrice': NewPrice, 
                    'OldPrice': OldPrice, 
                    'Size': Size, 
                    'Time': Time, 
                    'LastBump': LastBump, 
                    'Link': Link}
        if i%100 == 0:
            print(f'Completed row {i} out of {len(results)}')
    return pd.DataFrame(grailed_dict)

def page_dataframe(list_of_links, file_path):
    '''Iterates through a list of links generated by the make_dataframe function, loads individual
       pages, pulls additional features and saves the first image to the specified filepath. Images
       are named based on the index number of the link.
       
       A second DataFrame is output with the new features'''
    # Lists to append everything to
    UserName=[]
    Sold=[]
    Feedback=[]
    CurrentListings=[]
    Description=[]
    ProfileLink=[]
    FeedBack=[]
    FeedbackLink=[]
    FollowerCount=[]
    FullSize=[]
    PostedTime=[]
    BumpedTime=[]
    Location=[]
    Transactions=[]

    ### Iterating through the list of links, scraping additional data from each page and saving the first image
    for i, link in enumerate(df.Link):
        
        # navigating to the specified page
        driver.get(link)
        
        # saving the image by number of the Link index
        try:
            image = driver.find_element_by_xpath('//div[@class="-image-wrapper"]')
            src = image.find_element_by_tag_name('img').get_attribute('src')
            urllib.request.urlretrieve(src, f'{file_path}{i}.jpg')
        except NoSuchElementException:
            continue

        # saving seller's Username
        try:
            UserName.append(driver.find_element_by_xpath('//span[@class="-username"]').text)
        except NoSuchElementException:
            UserName.append(np.nan)

        # number of items seller has sold
        try:
            Sold.append(driver.find_element_by_xpath('//a[@class="-link"]/span[2]').text)
        except NoSuchElementException:
            Sold.append(np.nan)

        # Feedback rating
        try:
            FeedBack.append(driver.find_element_by_xpath('//span[@class="-feedback-count"]').text)
        except NoSuchElementException:
            FeedBack.append(np.nan)

        
        try:
            CurrentListings.append(driver.find_element_by_xpath('//a[@class="-for-sale-link"]').text)
        except NoSuchElementException:
            CurrentListings.append(np.nan)

        try:
            Description.append(driver.find_element_by_xpath('//div[@class="listing-description"]').text)
        except NoSuchElementException:
            Description.append(np.nan)

        try:
            ProfileLink.append(driver.find_element_by_xpath('//span[@class="Username"]/a').get_attribute("href"))
        except NoSuchElementException:
            ProfileLink.append(np.nan)

        try:
            FeedbackLink.append(driver.find_element_by_xpath('//div[@class="-details"]/a').get_attribute("href"))
        except NoSuchElementException:
            FeedbackLink.append(np.nan)

        try:
            FollowerCount.append(driver.find_element_by_xpath('//p[@class="-follower-count"]').text)
        except NoSuchElementException:
            FollowerCount.append(np.nan)

        try:
            FullSize.append(driver.find_element_by_xpath('//h2[@class="listing-size sub-title"]').text)
        except NoSuchElementException:
            FullSize.append(np.nan)

        try:
            PostedTime.append(driver.find_element_by_xpath('//div[@class="-metadata"]/span[2]').text)
        except NoSuchElementException:
            PostedTime.append(np.nan)

        try:
            BumpedTime.append(driver.find_element_by_xpath('//div[@class="-metadata"]/span[4]').text)
        except NoSuchElementException:
            BumpedTime.append(np.nan)

        try:
            Location.append(driver.find_element_by_xpath('//label[@class="--label"]').text)
        except NoSuchElementException:
            Location.append(np.nan)
        if i%100 == 0:
            print(f'Completed Page {i} out of {len(list_of_links)}.')
        
    page_dict = {'Username': UserName,
                     'Sold': Sold,
                     'Feedback': FeedBack,
                     'CurrentListings': CurrentListings, 
                     'Description': Description, 
                     'ProfileLink': ProfileLink, 
                     'FeedbackLink': FeedbackLink, 
                     'FollowerCount': FollowerCount,
                     'FullSize': FullSize,
                     'PostedTime': PostedTime,
                     'BumpedTime': BumpedTime, 
                     'Location': Location}
        
    return pd.DataFrame(page_dict)